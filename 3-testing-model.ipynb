{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from pickle files.\n"
     ]
    }
   ],
   "source": [
    "pickle_file = './data/index.pkl'\n",
    "\n",
    "with open(pickle_file, 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "\n",
    "input_token_index = loaded_data['input_token_index']\n",
    "target_token_index = loaded_data['target_token_index']\n",
    "\n",
    "pickle_file = './data/data.pkl'\n",
    "\n",
    "with open(pickle_file, 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "\n",
    "encoder_input_data = loaded_data['encoder_input_data']\n",
    "decoder_input_data = loaded_data['decoder_input_data']\n",
    "decoder_target_data = loaded_data['decoder_target_data']\n",
    "\n",
    "print(\"Data loaded from pickle files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = encoder_input_data[40000:]\n",
    "decoder_input_data = decoder_input_data[40000:]\n",
    "decoder_target_data = decoder_target_data[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_decoder_tokens = len(target_token_index)\n",
    "num_encoder_tokens = len(input_token_index)\n",
    "max_decoder_seq_length = decoder_input_data.shape[1]\n",
    "max_encoder_seq_length = encoder_input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model trained\n",
    "model = keras.models.load_model(\"./models/s2s_model.keras\")\n",
    "\n",
    "# encoder model\n",
    "encoder_inputs = model.input[0] \n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output \n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# decoder model (unique names for each input layer to avoid name conflicts)\n",
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens), name=\"decoder_input\")\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"decoder_state_input_h\")\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"decoder_state_input_c\")\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# LSTM and Dense layers from the original model\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# decoder model\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # first character of target sequence is the start character\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value, verbose=0\n",
    "        )\n",
    "\n",
    "        # sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # either hit max length or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Accuracy on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V4 :\n",
    "- Accuracy: 0.81, Test Accuracy: 0.82\n",
    "- Loss: 0.61, Test Loss: 0.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['input_layer_4', 'input_layer_5']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 92ms/step - accuracy: 0.8347 - loss: 0.5316\n",
      "Test Loss: 0.5305841565132141, Test Accuracy: 0.835210919380188\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with custom Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_string(input_string):\n",
    "  encoder_input_data = np.zeros(\n",
    "    (1, max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype=\"float32\",\n",
    "  )\n",
    "  t = 0\n",
    "  for char in input_string:\n",
    "    encoder_input_data[0, t, input_token_index[char]] = 1.0\n",
    "    t += 1\n",
    "  encoder_input_data[0, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "\n",
    "  return encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: i can read.\n",
      "Decoded sentence: je peux te marier.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string_to_translate = \"i can read.\"\n",
    "\n",
    "input_seq = encode_string(string_to_translate)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print(\"Input sentence:\", string_to_translate)\n",
    "print(\"Decoded sentence:\", decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
